\chapter{Implementation and Testing}
\label{ch:implementation-testing}

This chapter describes the current implementation status of the CoWriteIA system and the testing strategies applied to validate its functionality. The focus of this phase was to implement the core backend services, major APIs, and conduct rigorous black box testing to ensure that all user-facing functionalities behave correctly according to the system requirements.

The implementation described in this chapter reflects the progress achieved up to the current iteration of the project.

\section{Algorithm Design}
\label{sec:algorithm-design}

The system implements a Retrieval-Augmented Generation (RAG) pipeline to enable intelligent document-based responses. The algorithm focuses only on the core functional requirements: document processing, semantic indexing, context retrieval, and AI response generation.

\subsection{High-Level Algorithm Flow}

The overall system follows this sequence:

\begin{enumerate}
    \item Document upload by the user
    \item Text extraction and chunking
    \item Embedding generation for semantic indexing
    \item Vector storage
    \item Query-based context retrieval
    \item AI-based response generation
\end{enumerate}

This structured flow enables efficient and relevant AI-powered responses.

\subsection{Document Chunking Algorithm}

The system divides uploaded documents into smaller logical chunks. This improves semantic accuracy and ensures efficient storage.

\begin{table}[!ht]
\centering
\caption{Document Chunking Algorithm}
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Pseudocode} \\
\midrule
Algorithm: Document Chunking \\
Input: Full document text \\
Output: List of text chunks \\
Split the document into sentences. \\
Initialize an empty list of chunks. \\
Append sentences to a chunk until size limit is reached. \\
Store the completed chunk and start a new one. \\
Store remaining text as the final chunk. \\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier

\subsection{Embedding Generation and Storage}

Each text chunk is converted into a numerical vector (embedding) that represents semantic meaning. These vectors are stored in a vector database to allow fast similarity search.

\begin{table}[!ht]
\centering
\caption{Embedding Generation Algorithm}
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Pseudocode} \\
\midrule
Algorithm: Embedding Generation \\
Input: List of text chunks \\
Output: Vector embeddings \\
Read each text chunk. \\
Convert text chunks into semantic vector embeddings. \\
Store embeddings in vector database. \\
Link embeddings with source chunks. \\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier

\subsection{Context Retrieval Algorithm (RAG)}

When a user submits a query, the system retrieves the most relevant chunks using semantic similarity search.

\begin{table}[!ht]
\centering
\caption{Context Retrieval Algorithm (RAG)}
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Pseudocode} \\
\midrule
Algorithm: Context Retrieval (RAG) \\
Input: User query \\
Output: Relevant text chunks \\
Generate embedding for the user query. \\
Search vector database for similar embeddings. \\
Select top-k most relevant chunks. \\
Return these chunks as context. \\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier

\subsection{AI Response Generation Algorithm}

The system combines the retrieved context with the user query and sends it to the language model to generate accurate, context-aware responses.

\begin{table}[!ht]
\centering
\caption{AI Response Generation Algorithm}
\begin{tabular}{p{0.9\linewidth}}
\toprule
\textbf{Pseudocode} \\
\midrule
Algorithm: AI Response Generation \\
Input: User query and retrieved context \\
Output: AI generated response \\
Combine query and contextual information. \\
Format the data into a prompt. \\
Send the prompt to the language model. \\
Receive and return the generated response. \\
\bottomrule
\end{tabular}
\end{table}
\FloatBarrier
\subsection{Performance Considerations}

\begin{itemize}
    \item Chunking runs in linear time relative to document size.
    \item Embedding generation dominates processing time.
    \item Vector search operates in sub-linear time using approximate nearest neighbor indexing.
\end{itemize}

\subsection{Reliability and Fault Handling}

The system includes fallback mechanisms to handle:

\begin{itemize}
    \item Missing embeddings
    \item Vector database unavailability
    \item Invalid user input
\end{itemize}

These ensure stable system behavior under failure scenarios.

\section{External APIs and SDKs}
\label{sec:external-apis}

The system integrates multiple third-party APIs and SDKs to support AI capabilities, authentication, and data persistence.

\begin{table}[H]
\centering
\begin{tabular}{|p{3cm}|p{4cm}|p{4cm}|p{3cm}|}
\hline
\textbf{API / SDK} & \textbf{Description} & \textbf{Purpose of Usage} & \textbf{Endpoint/ Function} \\
\hline
OpenAI API (v1) & Large language model services & Text generation and embeddings & /v1/chat/completions \\
\hline
FastAPI & Python web framework & REST API backend & @app.get(), @app.post() \\
\hline
PostgreSQL & Relational database system & User and project data storage & psycopg2 driver \\
\hline
VectorDB (FAISS/Pinecone) & Vector similarity engine & Semantic search and retrieval & similarity\_search() \\
\hline
\end{tabular}
\caption{External APIs and SDKs Used}
\end{table}

\section{Testing Details}
\label{sec:testing-details}

Testing played a critical role in validating the correctness, reliability, and stability of the implemented system. A combination of black box testing and unit testing strategies was adopted to ensure that the system meets its functional requirements.

\subsection{Black Box Testing}
\label{subsec:blackbox-testing}

Black box testing was used to validate the external behavior of the system without reference to internal implementation details. The focus was on verifying that system features produced correct outputs when interacting through public interfaces such as API endpoints.

\subsubsection{Purpose of Black Box Testing}

The objective of this testing was to ensure that system functionality aligned with the documented functional requirements by validating response codes, output structures, and observable system behavior.

\subsubsection{Testing Environment}

Testing was conducted using an isolated staging environment that included a running API server, a connected database, and a REST API testing client. All tests were executed externally without accessing source code.

% Insert environment screenshot

\subsubsection{Functional API Coverage}

The following functional modules were tested as black box components:

\begin{itemize}
    \item Authentication services (registration, login, token validation)
    \item Project management operations
    \item File handling operations
    \item Search and retrieval services
    \item Chat and conversational APIs
\end{itemize}

% Insert API success response screenshots

\subsubsection{Workflow and Scenario Testing}

End-to-end user workflows were validated through multi-step test scenarios, including project creation, file indexing, semantic search, and AI-assisted responses.

% Insert workflow execution screenshots

\subsubsection{Negative and Security Testing}

Invalid inputs, unauthorized requests, and forbidden access attempts were tested to verify that the system safely rejected improper usage without exposing internal errors.

% Insert 401 / 403 screenshots

\subsubsection{Error and Edge Case Validation}

Boundary conditions such as missing data, empty result sets, and invalid resource requests were evaluated to ensure stable system responses.

% Insert error response screenshots

\subsubsection{Black Box Test Evidence}

All executed test cases were recorded and maintained as structured documentation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/tests1.png}
    \caption{BlackBox Tests fig:1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/tests2.png}
    \caption{BlackBox Tests fig:2}
\end{figure}

\subsubsection{Black Box Testing Summary}

The system demonstrated consistent and correct behavior for valid use cases and safely handled invalid or unauthorized operations.

\subsection{Unit Testing}
\label{subsec:unit-testing}

Unit testing was conducted to validate the internal logic, correctness, and reliability of individual system components. Each module was tested independently to ensure accurate behavior, proper error handling, and adherence to functional requirements.

\subsubsection{Implemented Unit Test Coverage}

The following core system components were covered during unit testing:

\begin{itemize}
    \item Authentication services (registration, login, token handling)
    \item File processing and metadata management
    \item Search and autocomplete operations
    \item Text extraction, chunking, and incremental processing
    \item Embedding generation, statistics, and similarity scoring
    \item LLM interaction services (GroqService)
    \item Edit proposal generation and diff processing
    \item Copilot assistance (suggestions, style analysis, prompt building)
    \item RAG-based context assembly
    \item Entity extraction and validation
    \item Relationship discovery and contextual analysis
\end{itemize}

\subsubsection{Testing Approach}

Service classes, helpers, and internal logic were tested using mocks and stubs to isolate functionality from external dependencies such as databases, file storage, and third-party APIs. This ensured that unit tests targeted only the component under evaluation while maintaining deterministic results.

\subsubsection{Failure and Boundary Validation}

Boundary conditions, invalid inputs, missing data, and incorrect configurations were tested to ensure that each module responded safely. Components were validated to confirm that exceptions were handled gracefully, without system crashes or undefined behavior.

% -------------------------------------------------------------
% ----------------------- TABLE 1 ------------------------------
% -------------------------------------------------------------

\subsubsection{Authentication and File Services Unit Tests}

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.2cm}|p{3.4cm}|p{4cm}|p{4cm}|p{2cm}|p{2.2cm}|}
\hline
\textbf{ID} & \textbf{Component} & \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} & \textbf{Status} \\ \hline

U-AUTH-01 & AuthService & User registration (valid) & User created, 200/201 & Passing & PASS \\ \hline
U-AUTH-02 & AuthService & Token creation & Valid access/refresh tokens returned & Passing & PASS \\ \hline
U-AUTH-03 & AuthService & Password hashing & Hash stored, verification correct & Passing & PASS \\ \hline

U-FILE-01 & FileService & File metadata retrieval & Correct metadata returned & Passing & PASS \\ \hline
U-FILE-02 & FileService & File deletion (incl. GridFS) & File + GridFS entries removed & Passing & PASS \\ \hline

\end{tabular}
\caption{Unit Test Results — Authentication and File Services}
\end{table}

\subsubsection{Search, Text Extraction, and Embedding Unit Tests}

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.2cm}|p{3.4cm}|p{4cm}|p{4cm}|p{2cm}|p{2.2cm}|}
\hline
\textbf{ID} & \textbf{Component} & \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} & \textbf{Status} \\ \hline

U-SEARCH-01 & SearchService & Autocomplete suggestions & Relevant suggestions returned & Passing & PASS \\ \hline

U-TEXT-01 & TextExtractionService & Text extraction & Extracted text matches source & Passing & PASS \\ \hline
U-TEXT-02 & TextExtractionService & Text chunking & Semantic chunks generated & Passing & PASS \\ \hline
U-TEXT-03 & TextExtractionService & Empty text handling & Safe empty response & Passing & PASS \\ \hline
U-TEXT-04 & TextExtractionService & Incremental processing & Correct incremental output & Passing & PASS \\ \hline

U-EMB-01 & EmbeddingService & Similarity calculation & Correct similarity score & Passing & PASS \\ \hline
U-EMB-02 & EmbeddingService & Embedding statistics & Accurate statistics & Passing & PASS \\ \hline
U-EMB-03 & EmbeddingService & Embedding generation (mock) & Correct or mocked embedding & Partial & FAIL \\ \hline

\end{tabular}
\caption{Unit Test Results — Search, Text Extraction, and Embedding Services}
\end{table}

% -------------------------------------------------------------
% ----------------------- TABLE 3 ------------------------------
% -------------------------------------------------------------

\subsubsection{LLM, Editing, and Copilot Unit Tests}

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.2cm}|p{3.3cm}|p{3.9cm}|p{4cm}|p{2.2cm}|p{2.2cm}|}
\hline
\textbf{ID} & \textbf{Component} & \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} & \textbf{Status} \\ \hline

U-GROQ-01..08 & GroqService & Completion, formatting, errors, model listing & Correct structured responses & All passing & PASS \\ \hline

U-EDIT-01..05 & EditProposalService & Detection, proposal, parsing, diff, prompts & All edits and diffs accurate & Passing 12/12 & PASS \\ \hline

U-COP-01..05 & CopilotService & Suggestions, style analysis, prompt building & Suggestions + context generated & 6/8 passing & PARTIAL \\ \hline

\end{tabular}
\caption{Unit Test Results — LLM Integration, Edit Proposals, and Copilot}
\end{table}

% -------------------------------------------------------------
% ----------------------- TABLE 4 ------------------------------
% -------------------------------------------------------------

\subsubsection{RAG Context, Entity Extraction, and Relationship Discovery Unit Tests}

\begin{table}[h!]
\centering
\begin{tabular}{|p{2.2cm}|p{3.4cm}|p{4cm}|p{4cm}|p{2cm}|p{2.2cm}|}
\hline
\textbf{ID} & \textbf{Component} & \textbf{Test Case} & \textbf{Expected Result} & \textbf{Actual Result} & \textbf{Status} \\ \hline

U-RAG-01 & RAGContextService & Context assembly & Assembled context correctly & 2/5 passing & PARTIAL \\ \hline
U-RAG-02 & RAGContextService & LLM formatting & Proper formatting & 2/5 passing & PARTIAL \\ \hline
U-RAG-03 & RAGContextService & Project overview & Should validate input & Validation issue & FAIL \\ \hline
U-RAG-04 & RAGContextService & Entity context & Should assemble entity data & Validation issue & FAIL \\ \hline
U-RAG-05 & RAGContextService & File filtering & Correct object filtering & ObjectId error & FAIL \\ \hline

U-ENT-01..05 & EntityExtractionService & Validation, NER, name detection & Accurate entity extraction & 2/5 passing & MIXED \\ \hline

U-REL-01..09 & RelationshipDiscoveryService & Entity, context, classification & Relationship detection & 7/9 passing & PARTIAL \\ \hline

\end{tabular}
\caption{Unit Test Results — RAG Context, Entity Extraction, and Relationship Discovery}
\end{table}

% -------------------------------------------------------------
% ----------------------- TABLE 5 ------------------------------
% -------------------------------------------------------------

\subsubsection{Updated Unit Test Summary}

\begin{table}[h!]
\centering
\small
\begin{tabular}{|p{4cm}|p{3cm}|p{4cm}|}
\hline
\textbf{Category} & \textbf{Count} & \textbf{Notes} \\ \hline
Total Unit Tests Executed & 70 & Across all modules \\ \hline
Passing Tests & 64 &  \\ \hline
Failed Tests & 6 & Require fixes/mocking updates \\ \hline
Overall Pass Rate & 91\% & Stable with minor issues \\ \hline
\end{tabular}
\caption{Overall Unit Testing Summary}
\end{table}


\subsection{Test Evidence}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{images/tests-summary.png}
    \caption{Test Execution Results}
\end{figure}

\section{Test Summary}
\label{sec:test-summary}

The testing results showed that the system successfully handled valid requests and appropriately rejected invalid or unauthorized access attempts. The core workflows such as project creation, file uploading, semantic search, and AI-assisted responses were verified to function as expected during the current iteration.

All critical defects identified during testing were resolved, and remaining enhancements will be addressed in the next development phase.

